# Word2Vec: Custom Model Training with Gensim + NLTK

This project demonstrates how to train a Word2Vec model on a custom corpus using Python's **Gensim** and **NLTK** libraries.

## Features:
- Tokenizes a text corpus
- Trains a Word2Vec embedding model
- Retrieves vector representations
- Finds similar words based on embeddings

## Sample Output:
Vector for 'nlp':
 [-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496
 -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629
 -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886
  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067
  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943
  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532
  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452
 -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772
  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793
  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216
 -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176
 -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448
 -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063
  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279
  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635
 -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958
 -0.00903996  0.005836    0.00939121  0.00350693]

Words similar to 'nlp': [('.', 0.21616892516613007), ('learning', 0.044689226895570755), ('natural', 0.02019743248820305), ('word', 0.015017164871096611), ('helps', 0.010695204138755798), ('in', 0.006408605258911848), ('exciting', 0.0019510718993842602), ('and', -0.03284316137433052), ('word2vec', -0.04552798718214035), ('related', -0.045689091086387634)]